---
layout: post
title: 06/28/22
use_math: true
category: journal
---

<img src="/osunotebook/journal/images/Temp_Gain_Init_Temp.png" style="width: 90%;"/>

This image shows the amount of heating (eV) in 1 ps of simulation time. 

The parameters are as follows: 

* Nx = Ny = 20
* PPC = 10
* Triangle Shape Function
* Periodic BC
* n = 1e23 electrons/cc
* $T_0 = {1, 10, 100, 1000}$ eV
* $\Delta x = {0.2, 0.5, 0.8, 2.0, 3.0, 4.0, 5.0}$ nm

Seems that when The Debye Length is close to the cell size, the initial temperature does not matter too much. However, when the Debye Length is severely under-resolved it starts to matter more.  

<img src="/osunotebook/journal/images/Heat_Delay_2nm.png" style="width: 45%;"/> <img src="/osunotebook/journal/images/Heat_Delay_5nm.png" style="width: 45%;"/>

These pictures show how the heating seems to have the same rate of change regardless of initial temperature. However, we see at lower temperatures, there is a longer delay until when linear heating is reached. Maybe this is because the thermal velocity of electrons is too low for the low temperature simulations and is not able to sufficiently move around in the simulation volume in the given amount of time to cause heating. This could be related to some relevant timescale $t \sim \frac{\lambda_D}{v_\text{th}}$.

![image](https://user-images.githubusercontent.com/98538788/176775250-66e03c40-efcc-44ce-a30a-813793561b90.png)

Above I plotted an estimate of this delay time as a function of the cell size. It seems that there is roughly a linear relationship. However, if there is a lower (1/10) density, the linear relationship does not seem so clear. 

![image](https://user-images.githubusercontent.com/98538788/176775487-686b0037-17e5-4df0-8956-55bf3a613312.png)

I additionally tried varying the number of particles per cell. The Arber Formula predicts that $\frac{dT}{dt} \propto \frac{1}{n_\text{ppc}}$. I ran simulations with the same setup but with the following modifications:

* PPC = ${10, 40, 80, 150, 250}$
* $T_0 = {10, 100}$ eV
* $\Delta x = {1.0, 5.0}$ nm

Below, I have the heating results

![image](https://user-images.githubusercontent.com/98538788/176776159-3745f832-9049-42e6-83e5-59bb54626902.png)

It seems that the heating rate is not linear when the Debye-Length is severely under-resolved. 

![image](https://user-images.githubusercontent.com/98538788/176776361-32565245-8c6f-4db0-aae3-a632c0fe5b7a.png)

Additionally, above, I have a Log-Log Plot of the Temperature Gain with respect to the Particles Per Cell. This shows a clear inverse relationship of heating rate with particles per cell as one would expect (the slope gives us our relationship and should be close to 1). However, it looks like when the Debye-Length is very under-resolved, the $\frac{dT}{dt}$  is not exactly proportional to $\frac{1}{n_\text{ppc}}$. It seems like we can only assume this proportionality when the heating is purely linear. 

![image](https://user-images.githubusercontent.com/98538788/176778439-88675659-0cf8-4d97-9869-23f0b340dff6.png)

Furthermore, it seems like the delay time is affected by the number of particles per cell as well. So, the delay time is not just a function of the resolution.

Things to do: 

1. Try to understand how Hockney came up with heating time estimates from *Computer Simulation using Particles*
2. Look more into what this delay timescale might be
3. Continue working on other simulations modifying other parameters
4. Work on Modifying Gaussian Laser Beam Model to fit my data
    
