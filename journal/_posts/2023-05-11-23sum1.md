---
layout: post
title: Change Log Scaling, NN Noise
use_math: true
category: journal
---


# Modifying Tom's Log Scaling
In Tom's Process class, he pre-processed the intensity and three output energies in the following way. First, he divided each data point value by some scale factor that is somewhat around the typical values in the dataset. As an example from his file $\texttt{Model_Tuning_OSC_35_Epochs_CSV.py}$, he used ${1e19, 1e0, 1e8, 1e-1}$ for the intensity, max energy, total energy, and average energy scales. Then he simply took a natural logarithm of the data. The other two input variables (thickness and focal distance) were left unchanged. To me, this seems a problem because it is not very standardized. The typical approach to preprocessing involves something like $\texttt{StandardScaler}$ which normalizes the data points by subtracting by a mean and dividing by a standard deviation. If we are going to log the dataset, we should do it before the standard scaler transformation is applied. I implemented some different versions of this scaling and I tested them. I trained a 10,000 point dataset (8,000 training, 2,000 testing) with GP and compared the percent error with each method. 

First, I used Tom's Scaling

![pct_error](https://github.com/ronak-n-desai/osunotebook/assets/98538788/cca1f6bc-f286-493d-a05e-141b4759dc66)

Then, I took a logarithm of the intensity, and then applied standard scaler to all inputs/outputs

![pct_error](https://github.com/ronak-n-desai/osunotebook/assets/98538788/b74d2070-8951-4642-8b3e-fc26594ec526)

Finally, I just applied standard scaler to all inputs/outputs without taking any logarithms.

![pct_error](https://github.com/ronak-n-desai/osunotebook/assets/98538788/df063a3d-7efb-437f-a4f8-fadcb6afc435)

It appears that Tom's way of scaling the data performs worse. As a result, I think just sticking to the standard scaler preprocessing should work fine for the most part. Now, I have implemented multiple different ways of preprocessing, so I can always change how I do it in the future. 

# Testing Data

In Tom's Code, he only uses 10 percent of the testing data when checking error metrics

![image](https://github.com/ronak-n-desai/osunotebook/assets/98538788/b399b00c-8b65-4f01-81c8-87e5b835392a)

I just changed the code so that it uses the full testing dataset. This explains why sometimes I get lower accuracy when I have a high training percentage split. A low testing percentage split combined with only using 10% of the testing data means that we may not be using an adequate number of testing points in the error calculations. 

# NN Noise
The Gaussian Process has an inbuilt measure of the noise which is just the 


# Things to Do
- Item 1
- Item 2
