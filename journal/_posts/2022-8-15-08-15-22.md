---
layout: post
title: 08/15/22
use_math: true
category: journal
---


# Peculiarity of 10eV Sim (Cont)

Last week I tried to understand why exactly the 10eV simulations did not seem to increase heating as we increased the cell size. I decided to run some sims with 100eV for higher grid spacings: ${12, 15, 18, 25}$ nm.

![image](https://user-images.githubusercontent.com/98538788/184934724-e05e83c1-4288-4354-ad44-70c1467fbc16.png)

From these results, it is clear that somewhere around 10nm, this relationship breaks down. For the 10eV sims from last week, it broke down around 3nm. If I were to guess, this relationship would breakdown for 1000eV around $\sqrt{10} * 10 \approx 31$ nm and for 10000eV around $100 nm$. 

Here, is a plot with 1000eV

![image](https://user-images.githubusercontent.com/98538788/184948883-86087cf5-d7a1-4a58-89e4-c821fdaf2fd2.png)

In this plot, we start seeing the slope peak and stay constant around 25 nm. This tells me that my hunch might be correct and that there is some limit to the scaling of the heating with $\Delta x$. I predicted that it would be proportional to $\sqrt{T_0}$ because the Debye Length is proportional to $\sqrt{T_0}$. So, maybe, when the ratio of the Debye Length to the Grid Spacing goes below a certain amount, we no longer get the $\Delta x^2$ scaling. According to my results, this would suggest that $\lambda_D / \Delta x \sim 0.03$ is the limit. 

# Download of Pytorch

Currently, Pytorch only supports python 3.7-3.9. By default, python 3.9 is not available on OSC, but Tom has it downloaded. So, we can use

> source /fs/project/PAS1066/zhang_anaconda/anaconda3/bin/activate

to activate conda that has python 3.9 available.

## Conda Environment

> conda env list

This will list the already created conda environments. To remove an environment, type: 

> conda env remove -n *env-name*

To create a new environment (let's call it *pytorch-env*):

> conda create -n pytorch-env python=3.9
> 
> conda activate pytorch-env

Then, we can install some supporting packages

> conda install h5py imageio jupyter matplotlib numpy tqdm

Then, we should follow instruction on https://pytorch.org/get-started/locally/ to download pytorch. If just trying to run on CPU, should download using the 'cpuonly' flag. When downloading on OSC or Unity (or anywhere with CUDA Availability), we should check the CUDA version. to do this, we can type

> ls /usr/local/cuda

which should list all available versions of CUDA. At the moment, OSC doesn't seem to have CUDA 11.3, so it looks like I should use CUDA 10.2. From the pytorch website, I would download this using

> conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch

To test if CUDA is installed correctly, we can type: 

> python()
> 
> import torch
> 
> torch.cuda.is_available()

This should return true, however on OSC and Unity, we must specifically specify the GPU through a batch script or interactive session in order to return true. Ricky [has a page on LASERDOCS](https://u.osu.edu/laserdocs/sample-page/using-gpu-on-osc/) showing how to get the GPU stuff working on OSC and a sample batch script to run a test file. 

# Heating Time and Arber Formula

I've decided to do my best and finally try to figure out how Arber came up with the heating time formulas. The following is based on Chapter 9 in the book by Hockney which references his 1971 and 1974 papers. 

## Why Does Numerical Heating Happen?

In our PIC simulation, we compute the velocities at the next timestep $v_{i+1}$ by taking the velocity at the previous timestep $v_i$ and modifying it by some correction term that is proportional to the timestep length $\Delta t$. However, due to the fact that we use finite grid, approximate real particles with macro particles, use a finite time-step, errors in rounding, use of finite-difference, etc. we will develope some errors in our calculation of the (let's just consider electric) field $\delta E$. This would correspond to miscalculation in the force by $\delta F = e \delta E \Delta t$ which would deliver an impulse $m \delta v$. As a result, our error in calculating the velocity will be 

\begin{equation}
\delta v = \frac{e}{m} \Delta t \delta E
\end{equation}

Now, we can make an assumption that these errors in the calculation of fields will be distributed in a random fashion, so that we can treat each velocity deviation as a random walk in velocity space. If we consider $\Delta v$ as the total deviation of the calculated velocity from the true value, we should expect $\langle \Delta v \rangle = 0$ due to the symmetry of this random walk. However, the [squared deviations on average will increase over time](https://en.wikipedia.org/wiki/Random_walk). If we are considering $n$ time-steps (each with the same random error $\delta E$), we would have 

\begin{equation}
\langle \Delta v^2 \rangle = n \delta v ^2 = n \frac{e^2}{m^2} \Delta t^2 \delta E^2
\end{equation}

## Collision Time

The PIC simulations that Hockney is trying to simulate represent an ideal [Vlasov](https://en.wikipedia.org/wiki/Vlasov_equation) collisionless plasma. In an ideal, uniform, collisionless plasma, the orbits of ions and electrons are all straight lines. However, in the PIC simulation, errors in the calculations of velocities and positions will result in deviations from straight lines. Hockney defines the collision time $\tau_c$ as the time 






