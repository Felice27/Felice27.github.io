---
layout: post
title: Scaling GP Regression up
use_math: true
category: journal
---


# OSC GPUS

[OSC's Website](https://www.osc.edu/resources/technical_support/supercomputers/gpu_computing) lists the all the available GPUs. The default node is an Owens P100 which has 1 GPU per node and 16GB of memory per GPU. One way I can improve my results is by using better GPUs. The Pitzer Quad V100 has 4 GPUs per GPU node of each 32 GB memory and the newest Ascend A100 nodes have 4 GPUs per node with 300GB GPU memory per node. A guide on requesting resources is found [here](https://www.osc.edu/resources/technical_support/supercomputers/pitzer/guidance_on_requesting_resources_on_pitzer).

# Finding Theoretical MSE and MAPE

I modified the Fuchs data generation code to include columns for the noiseless max/total/avg energy. This way, I can compute a reference value for the MSE and Percent Error which would be exactly what the testing error should converge to. 

![image](https://user-images.githubusercontent.com/98538788/233175949-93664ca8-a55b-495a-9950-f402780966f6.png)

Above is an example with using GP on 1,000 data points where I have both the estimate in green (which is what I computed in last week's post for the 20,000 point dataset) and the "Ideal" value in blue which is just the error between the noiseless and the noisy energies. We see that these are very close, but I think it might be more sensible to use the "Ideal" value. 

# Trying to run 100000 data points in under 100 seconds

On my CPU, I can run 8000 data points in under 100 seconds using the settings I have been using with good accuracy

![image](https://user-images.githubusercontent.com/98538788/233184121-9cf7249b-cbda-4f87-b429-ff086579d345.png)

![image](https://user-images.githubusercontent.com/98538788/233184145-bddf3903-c91d-4e0c-bbf5-07dd8f4d4334.png)

Running this same code on 2 GPUs of each 32GB, I can run a maximum of around 40,000 data points in less than 26 seconds with good accuracy. However, the GP seems to limited in the amount of memory we can utilize. I had to use 2 GPUs just so that I wouldn't run out of memory. 

# Things to Do
- Figure out how to extract noise information from the Gaussian Process regression and if that is related to the amount of gasussian noise we are using.
- Find out how noise is extracted from the neural network or svr models. Stick to lower number of data points like 20,000
- Find out how would I update any of these models with new data, say 10,000 at a time.
